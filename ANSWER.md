**1)** Пусть на вход дан сигнал x[n], а на выход нужно дать два сигнала y1[n] и y2[n]:

```
 y1[n] = x[n - 1] + x[n] + x[n + 1]
 y2[n] = y2[n - 2] + y2[n - 1] + x[n]
```

Какой из двух сигналов будет проще и быстрее реализовать в модели массового параллелизма на GPU и почему?

***Ответ:** будет эффективнее реализовать первый вариант.*

*В первом случае можно загрузить всем `warp`-ом три кэш-линии `x`-ов (так как нужно еще и загружать `i-1` и `i+1`) и быстро посчитать все значения `y1`.*

*Во втором случае нужно ждать результаты вычисления предыдущих двух (в смысле глобальных индексов) потоков, что значит, что мы должны дожидаться окончания *всех* предыдущих потоков для вычисления текущего, что на самом деле быстрее будет сделать на процессоре.*

**2)** Предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Напоминание: инструкция исполняется (пусть и отмаскированно) в каждом потоке warp/wavefront если хотя бы один поток выполняет эту инструкцию неотмаскированно. Если не все потоки выполняют эту инструкцию неотмаскированно - происходит т.н. code divergence.

Пусть размер рабочей группы (32, 32, 1)

```
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);
if (idx % 32 < 16)
    foo();
else
    bar();
```

Произойдет ли code divergence? Почему?

***Ответ:** нет, не будет.*

*Так как размер `warp`-а равен 32, и размер рабочей группы (32,32,1), то вычислители при одном запуске `warp`-а будут получать одновременно одинаковый `get_local_id(1)` и разные (от 0 до 31) `get_local_id(0)`. Значение `get_local_size(1)` равно 32 исходя из размера рабочей группы. То есть, грубо говоря, при одном запуске вычислители получат `idx = const + 32 * [0:31]`, который при делении на `32` будет давать остаток от `const`. Получается, у всех потоков в одном `warp`-е будут оданаковые результаты `if`-а.*

**3)** Как и в прошлом задании предположим что размер warp/wavefront равен 32 и рабочая группа делится
на warp/wavefront-ы таким образом что внутри warp/wavefront
номер WorkItem по оси x меняется чаще всего, затем по оси y и затем по оси z.

Пусть размер рабочей группы (32, 32, 1).
Пусть data - указатель на массив float-данных в глобальной видеопамяти идеально выравненный (выравнен по 128 байтам, т.е. data % 128 == 0). И пусть размер кеш линии - 128 байт.

(a)
```
data[get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

***Ответ:** да, `32` кэш-линии.*

*Как и в предыдущей задаче индекс обращений ` = [0:31] + 32 * const`. То есть все потоки `warp`-а загружат ровно 1 кэш-линию и сделают записи ровно в ней.*

(b)
```
data[get_local_id(1) + get_local_size(1) * get_local_id(0)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

***Ответ:** нет, `32*32` кэш-линии.* 

*Индексы обращений ` = const + 32 * [0:31]`. То есть все получат разные кэш-линии, так как будут обращаться к байтам с индексами `const + 128 * i`, т.к. `sizeof(float) = 4`.*

(c)
```
data[1 + get_local_id(0) + get_local_size(0) * get_local_id(1)] = 1.0f;
```

Будет ли данное обращение к памяти coalesced? Сколько кеш линий записей произойдет в одной рабочей группе?

***Ответ:** да, `2*32` кэш-линии.*

*Индексы обращений ` = 1 + [0:31] + 32 * const`. Обращение будет coalesced, так как все обращаются в соседние индексы, но из-за сдвига 1 будет затронуто 2 кэш-линии, но в банки мы попадем разные.*
